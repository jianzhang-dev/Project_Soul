# rnn_baseline.py
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from torchvision import datasets, transforms
from torch.utils.data import DataLoader
import time
import numpy as np

# æ ‡å‡†RNNåŸºçº¿æ¨¡å‹
class RNNBaseline(nn.Module):
    def __init__(self, hidden_size=192, num_layers=2):
        super().__init__()
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        
        # è§†è§‰å‰ç«¯ (ä¸EmergentVisionä¿æŒä¸€è‡´)
        self.vision_frontend = nn.Sequential(
            nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, padding=1),
            nn.BatchNorm2d(32),
            nn.ReLU(),
            nn.MaxPool2d(kernel_size=2, stride=2), # 28x28 -> 14x14
            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1),
            nn.BatchNorm2d(64),
            nn.ReLU(),
            nn.MaxPool2d(kernel_size=2, stride=2), # 14x14 -> 7x7
            nn.Flatten(),
            nn.Linear(64 * 7 * 7, hidden_size)
        )
        
        # RNNæ ¸å¿ƒ (LSTM)
        self.rnn = nn.LSTM(
            input_size=hidden_size,
            hidden_size=hidden_size,
            num_layers=num_layers,
            batch_first=True,
            dropout=0.2 if num_layers > 1 else 0
        )
        
        # åˆ†ç±»å™¨
        self.classifier = nn.Linear(hidden_size, 10)
    
    def forward(self, x):
        # ç¼–ç å›¾åƒåˆ°ç‰¹å¾å‘é‡
        features = self.vision_frontend(x)  # [B, 360]
        
        # å°†é™æ€ç‰¹å¾è½¬æ¢ä¸ºæ—¶åºè¾“å…¥ (é‡å¤12æ¬¡)
        seq_features = features.unsqueeze(1).repeat(1, 12, 1)  # [B, 12, 360]
        
        # RNNå¤„ç†
        rnn_out, (h_n, c_n) = self.rnn(seq_features)
        
        # ä½¿ç”¨æœ€åæ—¶åˆ»çš„éšè—çŠ¶æ€åˆ†ç±»
        logits = self.classifier(h_n[-1])  # [B, 10]
        return logits

def probe_rnn_memory(model, device, total_steps=1000):

    print("\n" + "="*65)
    print("â³ å¼€å§‹è¿›è¡Œ RNN é•¿æœŸè®°å¿†æµ‹è¯•ï¼ˆä»…è¾“å…¥ä¸€æ¬¡ï¼Œè§‚å¯Ÿ1000æ­¥ï¼‰...")
    print("="*65)

    model.eval()
    batch_size = 1
    with torch.no_grad():
        # 1. æ„é€ æµ‹è¯•å›¾åƒ
        test_input = torch.randn(batch_size, 1, 28, 28, device=device)

        # 2. æå–ç‰¹å¾ï¼ˆåªåšä¸€æ¬¡ï¼‰
        initial_features = model.vision_frontend(test_input)  # [1, hidden_size]

        # 3. åˆå§‹åŒ–éšè—çŠ¶æ€
        h = torch.zeros(model.num_layers, batch_size, model.hidden_size, device=device)
        c = torch.zeros(model.num_layers, batch_size, model.hidden_size, device=device)

        # 4. å­˜å‚¨æ´»è·ƒå€¼
        activity_log = []

        # 5. æ¨¡æ‹Ÿ 1000 æ­¥
        for t in range(total_steps):
            if t == 0:
                input_t = initial_features.unsqueeze(1)  # ç¬¬ä¸€æ­¥è¾“å…¥å›¾åƒ
            else:
                input_t = torch.zeros(batch_size, 1, model.hidden_size, device=device)

            # æ¨é€è¾“å…¥
            _, (h, c) = model.rnn(input_t, (h, c))

            # è®°å½•æœ€åä¸€å±‚éšè—çŠ¶æ€çš„ L1 æ´»è·ƒå€¼
            activity = h[-1].abs().sum().item()
            activity_log.append(activity)

    # 6. åˆ†æç»“æœ
    peak_activity = activity_log[0]
    mid_activity_mean = np.mean(activity_log[500:750])   # ä¸­æœŸï¼ˆ500~750ï¼‰
    late_activity_mean = np.mean(activity_log[750:])     # åæœŸï¼ˆ750~999ï¼‰
    final_activity = activity_log[-1]
    decay_ratio = final_activity / peak_activity if peak_activity > 0 else 0

    print(f"ğŸ“ˆ æ´»è·ƒå€¼æ—¥å¿— (å‰5æ­¥): {[f'{v:.2f}' for v in activity_log[:5]]}")
    print(f"ğŸ“ˆ æ´»è·ƒå€¼æ—¥å¿— (æœ€å5æ­¥): {[f'{v:.2f}' for v in activity_log[-5:]]}")
    print(f"\nğŸ“Š è¶…é•¿æœŸè®°å¿†åˆ†æ:")
    print(f"  - å³°å€¼æ´»è·ƒå€¼ (t=0): {peak_activity:.2f}")
    print(f"  - æœ€ç»ˆæ´»è·ƒå€¼ (t=999): {final_activity:.2f}")
    print(f"  - ä¸­æœŸå¹³å‡ (t=500~750): {mid_activity_mean:.2f}")
    print(f"  - åæœŸå¹³å‡ (t=750~999): {late_activity_mean:.2f}")
    print(f"  - è¡°å‡ç‡ (final/initial): {decay_ratio*100:.1f}%")

    # 7. åˆ¤æ–­é•¿æœŸç¨³å®šæ€§
    if late_activity_mean > 0.1 and decay_ratio > 0.5:
        print("\n  [ç»“è®º] ğŸŸ¢ å…·å¤‡**è¶…å¼ºé•¿æœŸè®°å¿†èƒ½åŠ›**ï¼")
        if abs(mid_activity_mean - late_activity_mean) < 0.5:
            print("     âœ… æ´»è·ƒå€¼é«˜åº¦ç¨³å®š â†’ å¯èƒ½æ˜¯æ•°å€¼é”å®š")
        else:
            print("     âš ï¸  æ´»è·ƒå€¼ç¼“æ…¢æ¼‚ç§» â†’ åŠ¨æ€ä¸ç¨³å®š")
    elif late_activity_mean > 0.01:
        print("\n  [ç»“è®º] ğŸŸ¡ è®°å¿†æŒç»­è¡°å‡ï¼Œæœªå®Œå…¨æ¶ˆå¤±")
        print("     â³ å¯èƒ½å­˜åœ¨æ•°å€¼æ¼‚ç§»æˆ–æ¢¯åº¦é¥±å’Œ")
    else:
        print("\n  [ç»“è®º] ğŸ”´ é•¿æœŸè®°å¿†å®Œå…¨æ¶ˆå¤±")
        print("     âŒ RNN æ— æ³•ç»´æŒè¶…é•¿æ—¶é—´è®°å¿†")

    print("="*65)
    return activity_log
def main():
    # è¶…å‚æ•° (ä¸EmergentVisionä¿æŒä¸€è‡´)
    EPOCHS = 10
    BATCH_SIZE = 128
    MAX_LEARNING_RATE = 0.001
    HIDDEN_SIZE = 192
    
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"--- RNNåŸºçº¿æ¨¡å‹è®­ç»ƒ ---")
    print(f"è®¾å¤‡: {device}\n")

    # æ•°æ®åŠ è½½ (ä¸EmergentVisionä¿æŒä¸€è‡´)
    print("æ­£åœ¨åŠ è½½ Fashion-MNIST æ•°æ®é›†...")
    
    train_transform = transforms.Compose([
        transforms.RandomHorizontalFlip(p=0.5),
        transforms.RandomRotation(10),
        transforms.ToTensor(),
        transforms.Normalize((0.2860,), (0.3530,))
    ])
    
    test_transform = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize((0.2860,), (0.3530,))
    ])
    
    train_dataset = datasets.FashionMNIST('data', train=True, download=True, transform=train_transform)
    test_dataset = datasets.FashionMNIST('data', train=False, transform=test_transform)
    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)
    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)
    print("æ•°æ®é›†åŠ è½½å®Œæ¯•ã€‚\n")

    # æ¨¡å‹åˆå§‹åŒ–
    print("æ­£åœ¨åˆå§‹åŒ– RNN åŸºçº¿æ¨¡å‹...")
    model = RNNBaseline(hidden_size=HIDDEN_SIZE).to(device)
    
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters(), lr=MAX_LEARNING_RATE)
    
    from torch.optim.lr_scheduler import OneCycleLR
    scheduler = OneCycleLR(
        optimizer, 
        max_lr=MAX_LEARNING_RATE,
        steps_per_epoch=len(train_loader),
        epochs=EPOCHS,
        pct_start=0.2
    )
    
    num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    print(f"æ¨¡å‹åˆå§‹åŒ–å®Œæ¯•ã€‚æ€»å¯è®­ç»ƒå‚æ•°: {num_params:,}\n")

    # è®­ç»ƒä¸è¯„ä¼°å¾ªç¯
    best_accuracy = 0.0
    for epoch in range(1, EPOCHS + 1):
        start_time = time.time()
        
        # è®­ç»ƒé˜¶æ®µ
        model.train()
        total_train_loss = 0
        for batch_idx, (data, target) in enumerate(train_loader):
            data, target = data.to(device), target.to(device)
            optimizer.zero_grad()
            output = model(data)
            loss = criterion(output, target)
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
            optimizer.step()
            scheduler.step()
            total_train_loss += loss.item()

        avg_train_loss = total_train_loss / len(train_loader)
        
        # è¯„ä¼°é˜¶æ®µ
        model.eval()
        correct = 0
        total = 0
        with torch.no_grad():
            for data, target in test_loader:
                data, target = data.to(device), target.to(device)
                output = model(data)
                _, predicted = torch.max(output.data, 1)
                total += target.size(0)
                correct += (predicted == target).sum().item()

        accuracy = 100 * correct / total
        epoch_time = time.time() - start_time
        
        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if accuracy > best_accuracy:
            best_accuracy = accuracy
            torch.save(model.state_dict(), 'rnn_baseline_best.pth')
        
        # æ‰“å°å‘¨æœŸæŠ¥å‘Š
        current_lr = optimizer.param_groups[0]['lr']
        print(f"Epoch {epoch}/{EPOCHS} | "
              f"Time: {epoch_time:.2f}s | "
              f"LR: {current_lr:.6f} | "
              f"Loss: {avg_train_loss:.4f} | "
              f"Acc: {accuracy:.2f}%")

    print(f"\nè®­ç»ƒå®Œæˆï¼")
    print(f"æœ€ä½³å‡†ç¡®ç‡: {best_accuracy:.2f}%")
    
    # æ´»è·ƒå€¼æµ‹è¯•
    probe_rnn_memory(model, device)

if __name__ == "__main__":
    main()
